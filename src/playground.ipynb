{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sql_metadata\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pygraphviz as pgv\n",
    "json_path = \"../config/data_io_format.json\"\n",
    "with open(json_path) as f:\n",
    "    io_format = json.load(f)\n",
    "\n",
    "\n",
    "def python_data_io_parser(py_ipynb_file_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        py_ipynb_file_path(str) : a full path of .py or .ipynb file to parse\n",
    "    Returns:\n",
    "        tuple(List,List): pair of inputed and outputed filenames lists\n",
    "    Note:\n",
    "        The input and output format are defined in /config/data_io_format.json.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(py_ipynb_file_path) as f:\n",
    "        code = f.read()\n",
    "    # remove space,indention etc...\n",
    "    code = re.sub(\"[\\s]+\", \"\", code)\n",
    "    # if notebook is inputed, then remove metadata\n",
    "    if \".ipynb\" in (py_ipynb_file_path):\n",
    "        code = re.sub(\"\\\\\\\\\\\"\", \"\\\"\", code)\n",
    "    input_patterns = io_format[\"python\"][\"input_pattern\"]\n",
    "    output_patterns = io_format[\"python\"][\"output_pattern\"]\n",
    "    suffix = io_format[\"python\"][\"suffix\"]\n",
    "\n",
    "    input_data = []\n",
    "    output_data = []\n",
    "\n",
    "    for input_pattern in input_patterns:\n",
    "        input_data.extend(re.findall(input_pattern+suffix, code))\n",
    "\n",
    "    for output_pattern in output_patterns:\n",
    "        output_data.extend(re.findall(output_pattern+suffix, code))\n",
    "    return (input_data, output_data)\n",
    "\n",
    "\n",
    "def sql_data_io_parser(sql_file_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sql_file_path(str) : a full path of .sql file to parse\n",
    "    Returns:\n",
    "        tuple(List,List): pair of inputed and outputed tables lists\n",
    "    Note:\n",
    "        The supported table identifing formats are depend on sql-metadata library\n",
    "    \"\"\"\n",
    "    with open(sql_file_path) as f:\n",
    "        code = f.read()\n",
    "\n",
    "    output_patterns = io_format[\"sql\"][\"output_pattern\"]\n",
    "    suffix = io_format[\"sql\"][\"suffix\"]\n",
    "\n",
    "    all_tables = sql_metadata.Parser(code).tables\n",
    "    output_tables = []\n",
    "    for output_pattern in output_patterns:\n",
    "        output_tables.extend(re.findall(output_pattern+suffix, code))\n",
    "    input_tables = [\n",
    "        table for table in all_tables if table not in output_tables]\n",
    "    return(input_tables, output_tables)\n",
    "\n",
    "\n",
    "class data_processing_flow_dag:\n",
    "    \"\"\" \n",
    "    This class conducts a DAG of data processing flow. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, repository_path,show_directory=True):\n",
    "        \"\"\"\n",
    "         show_directory: bool,default:True\n",
    "           if False,the directory of the file is ignored.\n",
    "        \"\"\"\n",
    "        self.show_directory = show_directory\n",
    "        self.repository_path = repository_path\n",
    "        self.py_graph=None\n",
    "        self.sql_graph=None\n",
    "\n",
    "    def build_py_graph(self):\n",
    "        \"\"\"\n",
    "        Build data processing flow graph according to py files\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.py_graph = pgv.AGraph(directed=True)\n",
    "        all_files = [p for p in glob.glob(\n",
    "            self.repository_path+'/**', recursive=True) if os.path.isfile(p)]\n",
    "        for file in all_files:\n",
    "            if Path(file).suffix in [\".py\",\".ipynb\"]:\n",
    "                input_data, output_data = python_data_io_parser(file)\n",
    "            else:\n",
    "                continue\n",
    "            #convert backslash in Windows path to forwardslask\n",
    "            file=file.replace(\"\\\\\",\"/\").replace(\"\\n\",\"/n\")\n",
    "            if self.show_directory == False:\n",
    "                file = Path(file).name\n",
    "                input_data = [Path(_).name for _ in input_data]\n",
    "                output_data = [Path(_).name for _ in output_data]\n",
    "\n",
    "            # add nodes and edges on graph\n",
    "            self.py_graph.add_node(file,style=\"filled\",fillcolor=\"#ffc0cb\",shape=\"box\")\n",
    "            self.py_graph.add_nodes_from(input_data+output_data,style=\"filled\",fillcolor=\"#87cefa\")\n",
    "            edge_in = [(i, file) for i in input_data]\n",
    "            edge_out = [(file, o) for o in output_data]\n",
    "            self.py_graph.add_edges_from(edge_in+edge_out)\n",
    "\n",
    "    def build_sql_graph(self):\n",
    "        \"\"\"\n",
    "        Build data processing flow graph according to sql files\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.sql_graph = pgv.AGraph(directed=True)\n",
    "        all_files = [p for p in glob.glob(\n",
    "            self.repository_path+'/**', recursive=True) if os.path.isfile(p)]\n",
    "        for file in all_files:\n",
    "            if Path(file).suffix in[\".sql\"]:\n",
    "                input_data, output_data = sql_data_io_parser(file)\n",
    "            else:\n",
    "                continue\n",
    "            # convert backslash in Windows path to forwardslask\n",
    "            file=file.replace(\"\\\\\",\"/\").replace(\"\\n\",\"/n\")\n",
    "            if self.show_directory == False:\n",
    "                file = Path(file).name\n",
    "                input_data = [Path(_).name for _ in input_data]\n",
    "                output_data = [Path(_).name for _ in output_data]\n",
    "\n",
    "            # add nodes and edges on graph\n",
    "            self.sql_graph.add_node(file,style=\"filled\",color=\"#ffc0cb\",shape=\"box\")\n",
    "            self.sql_graph.add_nodes_from(input_data+output_data,style=\"filled\",color=\"#87cefa\")\n",
    "            edge_in = [(i, file) for i in input_data]\n",
    "            edge_out = [(file, o) for o in output_data]\n",
    "            self.sql_graph.add_edges_from(edge_in+edge_out)\n",
    "\n",
    "    def draw_graphs(self,save_path=None):\n",
    "        \"\"\"\n",
    "        Draw data processing flow graph in which the color of file nodes and data nodes are distrinct with graphviz.\n",
    "        Default : The graph is saved at data_processing_flow_graph folder the repository directory\n",
    "        \"\"\"\n",
    "        if save_path ==None:\n",
    "            save_path=self.repository_path+\"/\"\n",
    "        if self.py_graph:\n",
    "            self.py_graph.draw(save_path+Path(self.repository_path).name+\"_py.svg\",prog=\"dot\")\n",
    "        if self.sql_graph:\n",
    "            self.sql_graph.draw(save_path+Path(self.repository_path).name+\"_sql.svg\",prog=\"dot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dpfv_lib\n",
    "\n",
    "hoge=dpfv_lib.data_processing_flow_dag(repository_path=\"../test_repository\",show_directory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoge.build_py_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoge.build_sql_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save path c:\\Users\\TONG QIJUN\\repository\\data_processing_flow_visualizer\\test_repository/\n"
     ]
    }
   ],
   "source": [
    "hoge.draw_graphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('../test_repository/notebooks/process2.ipynb', 'd.csv'),\n",
       " ('a.csv', '../test_repository/notebooks/process2.ipynb'),\n",
       " ('a.csv', '../test_repository/scripts/process1.py'),\n",
       " ('c.csv', '../test_repository/notebooks/process2.ipynb'),\n",
       " ('c.csv', '../test_repository/notebooks/process3.ipynb'),\n",
       " ('d.csv', '../test_repository/scripts/anaysis.py'),\n",
       " ('../test_repository/notebooks/process3.ipynb', 'merge.csv'),\n",
       " ('e.csv', '../test_repository/notebooks/process3.ipynb'),\n",
       " ('merge.csv', '../test_repository/scripts/anaysis.py'),\n",
       " ('../test_repository/scripts/anaysis.py', 'output.csv'),\n",
       " ('../test_repository/scripts/process1.py', 'c.csv'),\n",
       " ('b.csv', '../test_repository/scripts/process1.py')]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoge.py_graph.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.add_edges_from(hoge.py_graph.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nx.simple_cycles(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiDegreeView({'../test_repository/notebooks/process2.ipynb': 3, 'd.csv': 2, 'a.csv': 2, '../test_repository/scripts/process1.py': 3, 'c.csv': 3, '../test_repository/notebooks/process3.ipynb': 3, '../test_repository/scripts/anaysis.py': 3, 'merge.csv': 2, 'e.csv': 1, 'output.csv': 1, 'b.csv': 1})"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../test_repository/notebooks/process2.ipynb ['a.csv', 'c.csv']\n",
      "d.csv ['../test_repository/notebooks/process2.ipynb']\n",
      "a.csv []\n",
      "../test_repository/scripts/process1.py ['a.csv', 'b.csv']\n",
      "c.csv ['../test_repository/scripts/process1.py']\n",
      "../test_repository/notebooks/process3.ipynb ['c.csv', 'e.csv']\n",
      "../test_repository/scripts/anaysis.py ['d.csv', 'merge.csv']\n",
      "merge.csv ['../test_repository/notebooks/process3.ipynb']\n",
      "e.csv []\n",
      "output.csv ['../test_repository/scripts/anaysis.py']\n",
      "b.csv []\n"
     ]
    }
   ],
   "source": [
    "for node in a.nodes():\n",
    "    if not node.endswith(\".sql\"):\n",
    "        if list(a.predecessors(node))>1:\n",
    "            print(\"WARNING:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"abc\".endswith((\"c\",\"d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dwh_mng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5cb22dd69ac50d65c4b4ebc000b411f9b7bdd4b2f29619b98e44f3cafb36b0df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
